Science builds on the past work of others. Researchers draw from prior work to synthesize existing knowledge, identify research opportunities, and find inspirations for future research. One of the fundamental ways researchers explore and learn from the literature is by reading scientific papers. This not only provides them insights into individual prior work, but the related work sections also allows scholars to discover and draw connections to additional relevant papers via inline citations [33 ]. This process allows researchers to contextualize the paper they are reading within cited work, become aware of research threads that influenced the current paper, and discover other important and relevant papers to further their literature reviews [23 , 33, 58 ]. Inline citations are a key resource for discovering papers. The behavior of following multiple levels of inline citations, sometimes referred to as chaining or footnote chasing, has been observed across many scholar groups such as sociology, computer science, and economics (summarized in [ 44 ]). More specifically, one survey study estimated that inline citations accounted for around one in five (21%) of paper discoveries during research [33]. While inline citations are useful for discovering literature, it is often difficult to prioritize which citations to pay attention to in the middle of a reading task. One challenge is that even though there is some relationship between all inline citations and the citing paper, only a subset of them will be relevant to the reader’s interests at the time of reading. This is especially challenging during literature reviews, where users need to read and skim many papers, each of which may contain dozens or hundreds of inline citations. For example, a user interested in learning about text analysis techniques reading a paper about sentiment analysis on customer reviews might be interested in inline citations to prior work in natural language processing but not e-commerce marketing. Recently, research systems have been developed to help readers discover papers. HCI researchers have designed numerous standalone interactive paper discovery tools to support exploration of large corpora of papers (e.g., [ 12 , 26 , 47 ]). NLP researchers have developed technologies that analyze inline citations in a way that could be assistive to understanding those citations, for instance classifying their level of influence on the citing paper [60 ] or predicting their intent (e.g., whether the citation informs the methods, background, or results) [16]. What readers do not have, but could benefit from, are tools that provide in-situ support, within a paper, for the challenging task of understanding how citations relate to their own nuanced, evolving research interests and search history. Such an understanding of citations is necessary for deciding which of many citations are worth consulting. The purpose of this paper is to design and evaluate usable in-situ aids for prioritizing inline citations. The key insight motivating our eventual design for citation prioritization aids arose from need-finding interviews (described in Section 3): participants wished for a tool that helped them keep an eye out for prior work that is cited by multiple papers they had read in a literature review. To continue the scenario above, if a user noticed a paper cited from both a paper about aspect extraction on customer reviews and another paper about sentiment analysis on news articles, the cited paper was expected to be more relevant and salient to the reader’s interest of text analysis techniques. However, keeping track of which papers are cited by multiple papers during a literature review is impractical in current reading tools: papers use opaque identifiers for citations, like reference numbers or author-year abbreviations that differ across papers. Current reading tools do not keep track of which citations a reader has seen before (a basic affordance that sees widespread use in web browsers, which render hyperlinks in purple color when they have already been visited). Even if a reader does recognize a citation that they have seen in another, they likely will not be able to recall the context from which it was cited in other papers (e.g., which sections and the citing sentences), making it difficult to assess their importance and relevance across their corpus. These factors led participants in our preliminary interviews (described in a later section) to point a concern of “missing out” on prior work that is well-known and frequently cited by other researchers working on similar topics. In this paper, we introduce and explore the idea of a personalized paper reading experience that augments citations in a reading tool based on their connections to the current user. We developed a Chrome-extension PDF reader for scientific papers called CiteSee. Leveraging a user’s paper library, publication record, and reading history, CiteSee visually augments scientific papers to help users keep track of citations to known papers and prioritize their exploration to citations to unknown prior work that were likely relevant to their literature review topics (Figure 4). One key motivation here is that a user’s publications and paper libraries can potentially represent their longer-term research interests, and their recent paper reading history can potentially represent their fluid and shorter-term research interests, such as during literature reviews for new projects. In addition to visually augmenting inline citations, to help users better make sense of the cited papers, CiteSee keeps track of a consistent and personalized context of how different papers connect to the user’s previous activities, for example, reminding users of the context of how they discovered different papers saved in their library or how an inline citation was described by other papers in their reading history (Figure 2). The final design of CiteSee was driven by need-finding interviews with five researcher participants with varying research experiences (described in a later section), as well as several months of internal testing, design, and evaluation by the research team. The primary design challenge we addressed was to develop in-situ indicators that were simultaneously deeply informative about the contexts where a citation has been encountered before, while also being subtle, integrating into a paper reading experience without distracting or overwhelming the reader.
The exponential growth of scientific publication and increasing interdisciplinary nature of scientific progress makes it increasingly hard for scholars to keep up with the latest developments. Academic search engines, such as Google Scholar and Semantic Scholar help scholars discover research papers. Automated summarization for research papers helps scholars triage between research papers. But when it comes to actually reading research papers, the process, based on a static PDF format, has remained largely unchanged for many decades. This is a problem because digesting technical research papers is difficult. In contrast, interactive and personalized documents have seen significant adoption in domains outside of academic research. For example, news websites such as the New York Times often present interactive articles with explorable visualizations that allow readers to understand complex data in a personalized way. E-readers, such as the Kindle, provide in-situ context to help readers better comprehend complex documents, showing inline term definitions and tracking occurrence of characters in a long novel. While prior work has focused on authoring support tools that can reduce effort in creating interactive scientific documents, they have not seen widespread adoption due to a lack of incentive structure. Furthermore, millions of research papers are locked in the rigid and static PDF format, whose low-level syntax makes it extremely difficult for systems to access semantic content, augment interactivity, or even provide basic reading functionality for assistive tools like screen readers. Fortunately, recent work on layout-aware document parsing, and large language models show promise for accessing the content of PDF documents, and building systems that can better understand their semantics. This raises an exciting challenge: Can we create intelligent, interactive, and accessible reading interfaces for research papers, even atop existing PDFs? To explore this question, we present the Semantic Reader Project, a broad collaborative effort across multiple non-profit, industry, and academic institutions to create interactive, intelligent reading interfaces for research papers. This project consists of three pillars: research, product, and open science resources. On the research front, the Semantic Reader Project combines AI and HCI research to design novel, AI-powered interactive reading interfaces that address a variety of user challenges faced by today’s scholars. We developed research prototypes and conducted usability studies that clarify their benefits. On the product front, we are developing the Semantic Reader, a freely available reading interface that integrates features from research prototypes as they mature. Finally, we are developing and releasing open science resources that drive both the research and the product. These resources together open-source software, AI models, and open datasets to support continued work in this area. In this paper, we focus on summarizing our efforts under the research pillar of the Semantic Reader Project. We structure our discussion around five broad challenges faced by readers of research papers: Discovery: Following paper citations is one of the main strategies that scholars employ to discover additional relevant papers, but keeping track of the large numbers of citations can be overwhelming. In §2, we explore ways to visually augment research papers to help readers prioritize their paper exploration during literature reviews. Efficiency: The exponential growth of publication makes it difficult for scholars to keep up-to-date with the literature— scholars need to skim and read many papers while making sure they capture enough details in each. In §3, we explore how support for non-linear reading can help readers consume research papers more efficiently. Comprehension: Research papers can be dense and contain terms that are unfamiliar either because the author newly introduces them or assumes readers have prerequisite domain knowledge. In §4, we explore how providing in-situ definitions and summaries can benefit readers especially when reading outside of their domains. Synthesis: The sensemaking process of synthesizing knowledge scattered across multiple papers is effortful but important. It allows scholars to make connections between prior work and identify opportunities for future research. In §5, we explore how to help readers collect information from and make sense of many papers to gain better understanding of broad research topics. Accessibility: Static PDFs are an ill-suited format for many reading interfaces. For example, PDFs are notoriously incompatible with screen readers, and represent a significant barrier for blind and low vision readers. Furthermore, an increasing number of scholars access content on mobile devices, on which PDFs of papers are difficult to read. In §6, we explore methods for converting legacy papers to more accessible representations. We conclude by discussing ongoing research opportunities in both AI and HCI for developing the future of scholarly reading interfaces. We provide pointers to our production reading interface and associated open resources to invite the broader research community to join our effort.
The task of scientific claim verification (Wadden et al., 2020; Kotonya and Toni, 2020) aims to help system users assess the veracity of a scientific claim relative to a corpus of research literature. Most existing work and available datasets focus on verifying claims against a much more limited context—for instance, a single article or text snippet (Saakyan et al., 2021; Sarrouti et al., 2021; Kotonya and Toni, 2020) or a small, artificially-constructed collection of documents (Wadden et al., 2020). Current state-of-the-art models are able to achieve very strong performance on these datasets in some cases approaching human agreement (Wadden et al., 2022). This gives rise to the question of the scalability of scientific claim verification systems to realistic, open-domain settings that involve verifying claims against corpora containing hundreds of thousands of documents. In these cases, claim verification systems should assist users by identifying and categorizing all available documents that contain evidence supporting or refuting each claim (Fig. 1). However, evaluating system performance in this setting is difficult because exhaustive evidence annotation is infeasible, an issue analogous to evaluation challenges in information retrieval (IR). In this paper, we construct a new test collection for open-domain scientific claim verification, called SCIFACT-OPEN, which requires models to verify claims against evidence from both the SCIFACT (Wadden et al., 2020) collection, as well as additional evidence from a corpus of 500K scientific research abstracts. To avoid the burden of exhaustive annotation, we take inspiration from the pooling strategy (Sparck Jones and van Rijsbergen, 1975) popularized by the TREC competitions (Voorhees and Harman, 2005) and combine the predictions of several state-of-the-art scientific claim verification models—for each claim, abstracts that the models identify as likely to SUPPORT or REFUTE the claim are included as candidates for human annotation. Our main contributions and findings are as follows. (1) We introduce SCIFACT-OPEN, a new test collection for open-domain scientific claim verification, including 279 claims verified against evidence retrieved from a corpus of 500K abstracts. (2) We find that state-of-the-art models developed for SCIFACT perform substantially worse (at least 15 F1) in the open-domain setting, highlighting the need to improve upon the generalization capabilities of existing systems. (3) We identify and characterize new dataset phenomena that are likely to occur in real-world claim verification settings. These include mismatches between the specificity of a claim and a piece of evidence, and the presence of conflicting evidence (Fig. 1). With SCIFACT-OPEN, we introduce a challenging new test set for scientific claim verification that more closely approximates how the task might be performed in real-word settings. This dataset will allow for further study of claim-evidence phenomena and model generalizability as encountered in open-domain scientific claim verification.
